{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor: Fábio Miranda \n",
    "\n",
    "\n",
    "Alunos: Giulia Araujo Castro e Pedro Célia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador automático de sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy\n",
    "\n",
    "#Importando as bibliotecas necessárias\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[@giuacastro_]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.pass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-50460ad0b0c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Leitura do arquivo no formato JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'auth.pass'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Configurando a biblioteca\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.pass'"
     ]
    }
   ],
   "source": [
    "#Leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produto escolhido para análise:\n",
    "produto = 'Harry Potter'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 500\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capturando os dados do twitter \n",
    "\n",
    "#Cria um objeto para a captura\n",
    "#api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "#i = 1\n",
    "#msgs = []\n",
    "#for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "#    msgs.append(msg.full_text.lower())\n",
    "#    i += 1\n",
    "#    if i > n:\n",
    "#        break\n",
    "\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "#shuffle(msgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando em uma planilha no Excel\n",
    "\n",
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "#if not os.path.isfile('./{0}.xlsx'.format(produto)): \n",
    "#    Abre o arquivo para escrita\n",
    "#    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "#    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "#    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "#    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "#    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "#    fecha o arquivo\n",
    "#    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando um set para retirar os tweets repetidos do banco de dados\n",
    "\n",
    "#dados = pd.read_excel(\"Harry Potter.xlsx\")\n",
    "#dados2 = set(dados.Treinamento)\n",
    "#HP = pd.Series(list(dados2))\n",
    "#HP.to_excel(\"Harry_Potter.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando as bases obtidas\n",
    "import re \n",
    "\n",
    "def cleanup(text):\n",
    "    import string\n",
    "    punctuation = '[!\\-.:?;]' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, ' ', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo o excel final obtido\n",
    "data_raw = pd.read_excel(\"base_final.xlsx\", sheet_name = \"Treinamento\")\n",
    "#Nomeando a coluna com as classificações dadas\n",
    "data_raw = data_raw.rename(columns = {\"Unnamed: 1\": \"Classificação\"})\n",
    "\n",
    "#Acessando os tweets Irrelevantes (0) e os Relevantes (1)\n",
    "treinamento0 = data_raw.loc[data_raw[\"Classificação\"] == 0][\"Treinamento\"]\n",
    "treinamento1 = data_raw.loc[data_raw[\"Classificação\"] == 1][\"Treinamento\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando uma série com os dados gerais e com os tweets já classificados  \n",
    "Serie_data = pd.Series(data_raw[\"Treinamento\"])\n",
    "serie_um = pd.Series(treinamento1)\n",
    "serie_zero = pd.Series(treinamento0)\n",
    "\n",
    "#Atribuindo o valor 1 para cada tweet \n",
    "data_abs = Serie_data.value_counts()\n",
    "Tserie1 = serie_um.value_counts()\n",
    "Tserie0 = serie_zero.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unindo todos os tweets \n",
    "trei0 = \" \".join(treinamento0)\n",
    "trei1 = \" \".join(treinamento1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando os tweets \n",
    "zero_trei = cleanup(trei0.lower())\n",
    "um_trei = cleanup(trei1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividindo as séries por palavras \n",
    "Serie0 = pd.Series(zero_trei.split())\n",
    "Serie1 = pd.Series(um_trei.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chegando aos valores relativos de cada palavra \n",
    "Tabela0_relativa = Serie0.value_counts(True)\n",
    "Tabela1_relativa = Serie1.value_counts(True)\n",
    "Tabela0_abs = Serie0.value_counts()\n",
    "Tabela1_abs = Serie1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtendo as probabilidades de um tweet ser relevante ou não\n",
    "Prel = (Tserie1.sum())/(data_abs.sum())\n",
    "Pirr = (Tserie0.sum())/(data_abs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lista_relev = Tabela1_abs[1]\n",
    "d_lista_irrelev = Tabela0_abs[1]\n",
    "d = d_lista_relev+d_lista_irrelev\n",
    "\n",
    "def probabilidade(palavra):\n",
    "    if palavra in Tabela1_abs:\n",
    "        k = Tabela1_abs[palavra]\n",
    "    else:\n",
    "        k = 0\n",
    "    if palavra in Tabela0_abs:\n",
    "        J = Tabela0_abs[palavra]\n",
    "    else:\n",
    "        J = 0\n",
    "    prob_relev =( k +1)/(len(Serie1) + d)\n",
    "    prob_irrelev =(J + 1)/(len(Serie0) + d) \n",
    "    return [prob_relev, prob_irrelev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando o Classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo a tabela obtida\n",
    "data_teste = pd.read_excel(\"base_final.xlsx\", sheet_name = \"Teste\")\n",
    "#Nomeando a coluna com as classificações\n",
    "data_teste = data_teste.rename(columns = {\"Unnamed: 1\": \"Classificação\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpando os tweets de teste\n",
    "\n",
    "#Cria uma lista para adicionar os tweets limpos\n",
    "tweets_limpos = []\n",
    "#Chama a função cleanup\n",
    "for tweet in data_teste[\"Teste\"]:\n",
    "    tweets_limpos.append(cleanup(tweet)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificador \n",
    "\n",
    "#Lista para adicionarmos a classificação\n",
    "classificacao = []\n",
    "\n",
    "for frase in tweets_limpos:\n",
    "    #Separando o tweet em palavras\n",
    "    frasesp = frase.split()\n",
    "    #Cria uma lista de palavras para cada tweet\n",
    "    lista_palavras = []\n",
    "    \n",
    "    for palavra in frasesp:\n",
    "        #Adicionando as palavras na lista de palavras\n",
    "        lista_palavras.append(palavra)\n",
    "        #Criando as probabilidades de ser Relevante (Pr) ou Irrelevante (Pir)\n",
    "        Pr = 1\n",
    "        Pir = 1\n",
    "        \n",
    "        for pal in lista_palavras:\n",
    "            \n",
    "            #Se a palavra estiver presente nas duas tabelas\n",
    "            if pal in Tabela1_relativa and pal in Tabela0_relativa:\n",
    "                Pr *= Tabela1_relativa[pal]\n",
    "                Pir *= Tabela0_relativa[pal]\n",
    "            \n",
    "            #Se estiver apenas na tabela Irrelevante\n",
    "            elif pal in Tabela0_relativa:\n",
    "                Pir *= Tabela0_relativa[pal]\n",
    "                probabilidade(pal)[0]*Pr\n",
    "            \n",
    "            #Se estiver apenas na tabela Relevante\n",
    "            elif pal in Tabela1_relativa:\n",
    "                Pr *= Tabela1_relativa[pal]\n",
    "                probabilidade(pal)[1]*Pir\n",
    "                \n",
    "            #Se não estiver em nenhuma     \n",
    "            else:\n",
    "                probabilidade(pal)[0]*Pr\n",
    "                probabilidade(pal)[1]*Pir\n",
    "    \n",
    "    #Calcula a probablidade de ser Relevante ou Irrelevante\n",
    "    P_irrelevante = Pir * Pirr\n",
    "    P_relevante = Pr * Prel\n",
    "    \n",
    "    #Compara os valores obtidos e adiciona a classificação na lista \n",
    "    if P_irrelevante > P_relevante: \n",
    "        classificacao.append(0)\n",
    "    elif P_relevante > P_irrelevante:\n",
    "        classificacao.append(1)\n",
    "    else: \n",
    "        classificacao.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando a performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformando a tabela de classificação feita à mão em uma lista\n",
    "x = data_teste[\"Classificação\"]\n",
    "lista = []\n",
    "for e in x: \n",
    "    lista.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "#Comparando as classificações\n",
    "i = 0\n",
    "e = 0\n",
    "while i < len(lista): \n",
    "    if lista[i] == classificacao[i]:\n",
    "        #Obtendo o número de classificações corretas do classificador\n",
    "        e+=1\n",
    "    i+=1\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.45454545454545"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Porcentagem de acertos obtidos \n",
    "P = e/220\n",
    "\n",
    "P*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
